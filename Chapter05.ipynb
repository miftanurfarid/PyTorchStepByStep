{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning with PyTorch Step-by-Step: A Beginner's Guide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:80% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:80% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import google.colab\n",
    "    import requests\n",
    "    url = 'https://raw.githubusercontent.com/dvgodoy/PyTorchStepByStep/master/config.py'\n",
    "    r = requests.get(url, allow_redirects=True)\n",
    "    open('config.py', 'wb').write(r.content)    \n",
    "except ModuleNotFoundError:\n",
    "    pass\n",
    "\n",
    "from config import *\n",
    "config_chapter5()\n",
    "# This is needed to render the plots in this chapter\n",
    "from plots.chapter5 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.transforms import Compose, Normalize\n",
    "\n",
    "from data_generation.image_classification import generate_dataset\n",
    "from helpers import index_splitter, make_balanced_sampler\n",
    "from stepbystep.v1 import StepByStep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter / Kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./images/conv1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 6, 6)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "single = np.array(\n",
    "    [[[[5, 0, 8, 7, 8, 1],\n",
    "       [1, 9, 5, 0, 7, 7],\n",
    "       [6, 0, 2, 4, 6, 6],\n",
    "       [9, 7, 6, 6, 8, 4],\n",
    "       [8, 3, 8, 5, 1, 3],\n",
    "       [7, 2, 7, 0, 1, 0]]]]\n",
    ")\n",
    "single.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 3, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "identity = np.array(\n",
    "    [[[[0, 0, 0],\n",
    "       [0, 1, 0],\n",
    "       [0, 0, 0]]]]\n",
    ")\n",
    "identity.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./images/conv2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "region = single[:, :, 0:3, 0:3]\n",
    "filtered_region = region * identity\n",
    "total = filtered_region.sum()\n",
    "total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./images/conv3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Moving Around"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./images/stride1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_region = single[:, :, 0:3, (0+1):(3+1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./images/conv5.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_filtered_region = new_region * identity\n",
    "new_total = new_filtered_region.sum()\n",
    "new_total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./images/conv6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./images/conv7.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_horizontal_region = single[:, :, 0:3, (0+4):(3+4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (1,1,3,2) (1,1,3,3) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_75695/2080732797.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlast_horizontal_region\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0midentity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (1,1,3,2) (1,1,3,3) "
     ]
    }
   ],
   "source": [
    "last_horizontal_region * identity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./images/conv8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\Large\n",
    "(h_i, w_i) * (h_f, w_f) = (h_i - (h_f - 1), w_i - (w_f - 1))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\Large\n",
    "(h_i, w_i) * f = (h_i - f + 1, w_i - f + 1)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolving in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = torch.as_tensor(single).float()\n",
    "kernel_identity = torch.as_tensor(identity).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[9., 5., 0., 7.],\n",
       "          [0., 2., 4., 6.],\n",
       "          [7., 6., 6., 8.],\n",
       "          [3., 8., 5., 1.]]]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convolved = F.conv2d(image, kernel_identity, stride=1)\n",
    "convolved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.5369,  1.9690, -1.1619, -3.1844],\n",
       "          [-0.8648, -4.0165, -0.3939, -0.4451],\n",
       "          [-2.0863, -0.3478, -2.1432, -0.7654],\n",
       "          [-2.5016, -2.5363, -1.0745, -1.8194]]]],\n",
       "       grad_fn=<ConvolutionBackward0>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=3, stride=1)\n",
    "conv(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[[-0.0390,  0.0643,  0.2987],\n",
       "          [ 0.2798,  0.1787,  0.1787],\n",
       "          [-0.2616,  0.1644,  0.2201]]],\n",
       "\n",
       "\n",
       "        [[[ 0.2553, -0.0587,  0.0786],\n",
       "          [ 0.1484,  0.3274, -0.2807],\n",
       "          [-0.1516, -0.2343,  0.1902]]]], requires_grad=True)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_multiple = nn.Conv2d(in_channels=1, out_channels=2, kernel_size=3, stride=1)\n",
    "conv_multiple.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    conv.weight[0] = kernel_identity\n",
    "    conv.bias[0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[9., 5., 0., 7.],\n",
       "          [0., 2., 4., 6.],\n",
       "          [7., 6., 6., 8.],\n",
       "          [3., 8., 5., 1.]]]], grad_fn=<ConvolutionBackward0>)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Striding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./images/strider2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./images/strider3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\Large\n",
    "(h_i, w_i) * f = \\left(\\frac{h_i - f + 1}{s}, \\frac{w_i - f + 1}{s}\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[9., 0.],\n",
       "          [7., 6.]]]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convolved_stride2 = F.conv2d(image, kernel_identity, stride=2)\n",
    "convolved_stride2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./images/padding1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "          [0., 5., 0., 8., 7., 8., 1., 0.],\n",
       "          [0., 1., 9., 5., 0., 7., 7., 0.],\n",
       "          [0., 6., 0., 2., 4., 6., 6., 0.],\n",
       "          [0., 9., 7., 6., 6., 8., 4., 0.],\n",
       "          [0., 8., 3., 8., 5., 1., 3., 0.],\n",
       "          [0., 7., 2., 7., 0., 1., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0., 0.]]]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "constant_padder = nn.ConstantPad2d(padding=1, value=0)\n",
    "constant_padder(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded = F.pad(image, pad=(1, 1, 1, 1), mode='constant', value=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/paddings.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[5., 5., 0., 8., 7., 8., 1., 1.],\n",
       "          [5., 5., 0., 8., 7., 8., 1., 1.],\n",
       "          [1., 1., 9., 5., 0., 7., 7., 7.],\n",
       "          [6., 6., 0., 2., 4., 6., 6., 6.],\n",
       "          [9., 9., 7., 6., 6., 8., 4., 4.],\n",
       "          [8., 8., 3., 8., 5., 1., 3., 3.],\n",
       "          [7., 7., 2., 7., 0., 1., 0., 0.],\n",
       "          [7., 7., 2., 7., 0., 1., 0., 0.]]]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replication_padder = nn.ReplicationPad2d(padding=1)\n",
    "replication_padder(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[9., 1., 9., 5., 0., 7., 7., 7.],\n",
       "          [0., 5., 0., 8., 7., 8., 1., 8.],\n",
       "          [9., 1., 9., 5., 0., 7., 7., 7.],\n",
       "          [0., 6., 0., 2., 4., 6., 6., 6.],\n",
       "          [7., 9., 7., 6., 6., 8., 4., 8.],\n",
       "          [3., 8., 3., 8., 5., 1., 3., 1.],\n",
       "          [2., 7., 2., 7., 0., 1., 0., 1.],\n",
       "          [3., 8., 3., 8., 5., 1., 3., 1.]]]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reflection_padder = nn.ReflectionPad2d(padding=1)\n",
    "reflection_padder(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0., 7., 2., 7., 0., 1., 0., 7.],\n",
       "          [1., 5., 0., 8., 7., 8., 1., 5.],\n",
       "          [7., 1., 9., 5., 0., 7., 7., 1.],\n",
       "          [6., 6., 0., 2., 4., 6., 6., 6.],\n",
       "          [4., 9., 7., 6., 6., 8., 4., 9.],\n",
       "          [3., 8., 3., 8., 5., 1., 3., 8.],\n",
       "          [0., 7., 2., 7., 0., 1., 0., 7.],\n",
       "          [1., 5., 0., 8., 7., 8., 1., 5.]]]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.pad(image, pad=(1, 1, 1, 1), mode='circular')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\Large\n",
    "(h_i, w_i) * f = \\left(\\frac{(h_i + 2p) - f + 1}{s}, \\frac{(w_i + 2p) - f + 1}{s}\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A REAL Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 3, 3])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge = np.array(\n",
    "    [[[[0, 1, 0],\n",
    "       [1, -4, 1],\n",
    "       [0, 1, 0]]]]\n",
    ")\n",
    "kernel_edge = torch.as_tensor(edge).float()\n",
    "kernel_edge.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./images/padding2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./images/padding3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded = F.pad(image, (1, 1, 1, 1), mode='constant', value=0)\n",
    "conv_padded = F.conv2d(padded, kernel_edge, stride=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "          [0., 5., 0., 8., 7., 8., 1., 0.],\n",
       "          [0., 1., 9., 5., 0., 7., 7., 0.],\n",
       "          [0., 6., 0., 2., 4., 6., 6., 0.],\n",
       "          [0., 9., 7., 6., 6., 8., 4., 0.],\n",
       "          [0., 8., 3., 8., 5., 1., 3., 0.],\n",
       "          [0., 7., 2., 7., 0., 1., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0., 0.]]]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-19.,  22., -20., -12., -17.,  11.],\n",
       "          [ 16., -30.,  -1.,  23.,  -7., -14.],\n",
       "          [-14.,  24.,   7.,  -2.,   1.,  -7.],\n",
       "          [-15., -10.,  -1.,  -1., -15.,   1.],\n",
       "          [-13.,  13., -11.,  -5.,  13.,  -7.],\n",
       "          [-18.,   9., -18.,  13.,  -3.,   4.]]]])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_padded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./images/pooling1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[22., 23., 11.],\n",
       "          [24.,  7.,  1.],\n",
       "          [13., 13., 13.]]]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pooled = F.max_pool2d(conv_padded, kernel_size=2)\n",
    "pooled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[24.]]]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxpool4 = nn.MaxPool2d(kernel_size=4)\n",
    "pooled4 = maxpool4(conv_padded)\n",
    "pooled4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[24., 24., 23., 23.],\n",
       "          [24., 24., 23., 23.],\n",
       "          [24., 24., 13., 13.],\n",
       "          [13., 13., 13., 13.]]]])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.max_pool2d(conv_padded, kernel_size=3, stride=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flattening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[22., 23., 11., 24.,  7.,  1., 13., 13., 13.]])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flattened = nn.Flatten()(pooled)\n",
    "flattened"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[22., 23., 11., 24.,  7.,  1., 13., 13., 13.]])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pooled.view(1, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Typical Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LeNet5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/architecture_lenet.png)\n",
    "\n",
    "*Source: Generated using Alexander Lenail's [NN-SVG](http://alexlenail.me/NN-SVG/) and adapted by the author. For more details, see LeCun, Y., et al (1998).  [Gradient-based learning applied to document recognition](http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf). Proceedings of the IEEE,86(11), 2278–2324*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "lenet = nn.Sequential()\n",
    "\n",
    "# Featurizer\n",
    "# Block 1: 1@28x28 -> 6@28x28 -> 6@14x14\n",
    "lenet.add_module('C1', nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5, padding=2))\n",
    "lenet.add_module('func1', nn.ReLU())\n",
    "lenet.add_module('S2', nn.MaxPool2d(kernel_size=2))\n",
    "# Block 2: 6@14x14 -> 16@10x10 -> 16@5x5\n",
    "lenet.add_module('C3', nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5))\n",
    "lenet.add_module('func2', nn.ReLU())\n",
    "lenet.add_module('S4', nn.MaxPool2d(kernel_size=2))\n",
    "# Block 3: 16@5x5 -> 120@1x1\n",
    "lenet.add_module('C5', nn.Conv2d(in_channels=16, out_channels=120, kernel_size=5))\n",
    "lenet.add_module('func2', nn.ReLU())\n",
    "# Flattening\n",
    "lenet.add_module('flatten', nn.Flatten())\n",
    "\n",
    "# Classification\n",
    "# Hidden Layer\n",
    "lenet.add_module('F6', nn.Linear(in_features=120, out_features=84))\n",
    "lenet.add_module('func3', nn.ReLU())\n",
    "# Output Layer\n",
    "lenet.add_module('OUTPUT', nn.Linear(in_features=84, out_features=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Multiclass Classification Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = generate_dataset(img_size=10, n_images=1000, binary=False, seed=17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x450 with 30 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plot_images(images, labels, n_plot=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformedTensorDataset(Dataset):\n",
    "    def __init__(self, x, y, transform=None):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        x = self.x[index]\n",
    "        \n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "        \n",
    "        return x, self.y[index]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Builds tensors from numpy arrays BEFORE split\n",
    "# Modifies the scale of pixel values from [0, 255] to [0, 1]\n",
    "x_tensor = torch.as_tensor(images / 255).float()\n",
    "y_tensor = torch.as_tensor(labels).long()\n",
    "\n",
    "# Uses index_splitter to generate indices for training and\n",
    "# validation sets\n",
    "train_idx, val_idx = index_splitter(len(x_tensor), [80, 20])\n",
    "# Uses indices to perform the split\n",
    "x_train_tensor = x_tensor[train_idx]\n",
    "y_train_tensor = y_tensor[train_idx]\n",
    "x_val_tensor = x_tensor[val_idx]\n",
    "y_val_tensor = y_tensor[val_idx]\n",
    "\n",
    "# We're not doing any data augmentation now\n",
    "train_composer = Compose([Normalize(mean=(.5,), std=(.5,))])\n",
    "val_composer = Compose([Normalize(mean=(.5,), std=(.5,))])\n",
    "\n",
    "# Uses custom dataset to apply composed transforms to each set\n",
    "train_dataset = TransformedTensorDataset(x_train_tensor, y_train_tensor, transform=train_composer)\n",
    "val_dataset = TransformedTensorDataset(x_val_tensor, y_val_tensor, transform=val_composer)\n",
    "\n",
    "# Builds a weighted random sampler to handle imbalanced classes\n",
    "sampler = make_balanced_sampler(y_train_tensor)\n",
    "\n",
    "# Uses sampler in the training set to get a balanced data loader\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=16, sampler=sampler)\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\Large\n",
    "\\begin{aligned}\n",
    "z & = \\text{logit}(p) & &= \\text{log odds ratio }(p) & &= \\text{log}\\left(\\frac{p}{1-p}\\right) & \\\\\n",
    "e^z & = e^{\\text{logit}(p)} & &= \\text{odds ratio }(p) & &= \\left(\\frac{p}{1-p}\\right) &\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\Large\n",
    "\\text{softmax}(z_i) = \\frac{e^{z_i}}{\\sum_{c=0}^{C-1}{e^{z_c}}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\Large\n",
    "\\text{softmax}(z) = \\left[\\frac{e^{z_0}}{e^{z_0}+e^{z_1}+e^{z_2}},\\frac{e^{z_1}}{e^{z_0}+e^{z_1}+e^{z_2}},\\frac{e^{z_2}}{e^{z_0}+e^{z_1}+e^{z_2}}\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = torch.tensor([ 1.3863,  0.0000, -0.6931])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "odds_ratios = torch.exp(logits)\n",
    "odds_ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmaxed = odds_ratios / odds_ratios.sum()\n",
    "softmaxed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.Softmax(dim=-1)(logits), F.softmax(logits, dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Negative Log Likelihood Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\Large\n",
    "\\texttt{BCE}(y)={-\\frac{1}{(N_{\\text{pos}}+N_{\\text{neg}})}\\Bigg[{\\sum_{i=1}^{N_{\\text{pos}}}{\\text{log}(\\text{P}(y_i=1))} + \\sum_{i=1}^{N_{\\text{neg}}}{\\text{log}(1 - \\text{P}(y_i=1))}}\\Bigg]}\n",
    "\\\\\n",
    "\\Large\n",
    "\\texttt{NLLLoss}(y)={-\\frac{1}{(N_0+N_1+N_2)}\\Bigg[{\\sum_{i=1}^{N_0}{\\text{log}(\\text{P}(y_i=0))} + \\sum_{i=1}^{N_1}{\\text{log}(\\text{P}(y_i=1))} + \\sum_{i=1}^{N_2}{\\text{log}(\\text{P}(y_i=2))}}\\Bigg]}\n",
    "\\\\\n",
    "\\Large \\texttt{NLLLoss}(y)={-\\frac{1}{(N_0+\\cdots+N_{C-1})}\\sum_{c=0}^{C-1}{\\sum_{i=1}^{N_c}{\\text{log}(\\text{P}(y_i=c))} }}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_probs = F.log_softmax(logits, dim=-1)\n",
    "log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = torch.tensor([2])\n",
    "F.nll_loss(log_probs.view(-1, 3), label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(11)\n",
    "dummy_logits = torch.randn((5, 3))\n",
    "dummy_labels = torch.tensor([0, 0, 1, 2, 1])\n",
    "dummy_log_probs = F.log_softmax(dummy_logits, dim=-1)\n",
    "dummy_log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_log_probs = torch.tensor([-1.5229, -1.7934, -1.0136, -2.0367, -1.9098])\n",
    "-relevant_log_probs.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.NLLLoss()\n",
    "loss_fn(dummy_log_probs, dummy_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.NLLLoss(weight=torch.tensor([1., 1., 2.]))\n",
    "loss_fn(dummy_log_probs, dummy_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.NLLLoss(ignore_index=2)\n",
    "loss_fn(dummy_log_probs, dummy_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Entropy Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(11)\n",
    "dummy_logits = torch.randn((5, 3))\n",
    "dummy_labels = torch.tensor([0, 0, 1, 2, 1])\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "loss_fn(dummy_logits, dummy_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(13)\n",
    "model_cnn1 = nn.Sequential()\n",
    "\n",
    "# Featurizer\n",
    "# Block 1: 1@10x10 -> n_channels@8x8 -> n_channels@4x4\n",
    "n_channels = 1\n",
    "model_cnn1.add_module('conv1', nn.Conv2d(in_channels=1, out_channels=n_channels, kernel_size=3))\n",
    "model_cnn1.add_module('relu1', nn.ReLU())\n",
    "model_cnn1.add_module('maxp1', nn.MaxPool2d(kernel_size=2))\n",
    "# Flattening: n_channels * 4 * 4\n",
    "model_cnn1.add_module('flatten', nn.Flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification\n",
    "# Hidden Layer\n",
    "model_cnn1.add_module('fc1', nn.Linear(in_features=n_channels*4*4, out_features=10))\n",
    "model_cnn1.add_module('relu2', nn.ReLU())\n",
    "# Output Layer\n",
    "model_cnn1.add_module('fc2', nn.Linear(in_features=10, out_features=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/classification_softmax.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.1\n",
    "multi_loss_fn = nn.CrossEntropyLoss(reduction='mean')\n",
    "optimizer_cnn1 = optim.SGD(model_cnn1.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbs_cnn1 = StepByStep(model_cnn1, multi_loss_fn, optimizer_cnn1)\n",
    "sbs_cnn1.set_loaders(train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbs_cnn1.train(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = sbs_cnn1.plot_losses()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Filters and More!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@staticmethod\n",
    "def _visualize_tensors(axs, x, y=None, yhat=None, \n",
    "                       layer_name='', title=None):\n",
    "    # The number of images is the number of subplots in a row\n",
    "    n_images = len(axs)\n",
    "    # Gets max and min values for scaling the grayscale\n",
    "    minv, maxv = np.min(x[:n_images]), np.max(x[:n_images])\n",
    "    # For each image\n",
    "    for j, image in enumerate(x[:n_images]):\n",
    "        ax = axs[j]\n",
    "        # Sets title, labels, and removes ticks\n",
    "        if title is not None:\n",
    "            ax.set_title(f'{title} #{j}', fontsize=12)\n",
    "        shp = np.atleast_2d(image).shape\n",
    "        ax.set_ylabel(\n",
    "            f'{layer_name}\\n{shp[0]}x{shp[1]}',\n",
    "            rotation=0, labelpad=40\n",
    "        )\n",
    "        xlabel1 = '' if y is None else f'\\nLabel: {y[j]}'\n",
    "        xlabel2 = '' if yhat is None else f'\\nPredicted: {yhat[j]}'\n",
    "        xlabel = f'{xlabel1}{xlabel2}'\n",
    "        if len(xlabel):\n",
    "            ax.set_xlabel(xlabel, fontsize=12)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "\n",
    "        # Plots weight as an image\n",
    "        ax.imshow(\n",
    "            np.atleast_2d(image.squeeze()),\n",
    "            cmap='gray', \n",
    "            vmin=minv, \n",
    "            vmax=maxv\n",
    "        )\n",
    "    return\n",
    "\n",
    "setattr(StepByStep, '_visualize_tensors', _visualize_tensors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Static Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cat(object):\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "\n",
    "    @staticmethod\n",
    "    def meow():\n",
    "        print('Meow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cat.meow()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_filter = model_cnn1.conv1.weight.data.cpu().numpy()\n",
    "weights_filter.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_filters(self, layer_name, **kwargs):\n",
    "    try:\n",
    "        # Gets the layer object from the model\n",
    "        layer = self.model\n",
    "        for name in layer_name.split('.'):\n",
    "            layer = getattr(layer, name)\n",
    "        # We are only looking at filters for 2D convolutions\n",
    "        if isinstance(layer, nn.Conv2d):\n",
    "            # Takes the weight information\n",
    "            weights = layer.weight.data.cpu().numpy()\n",
    "            # weights -> (channels_out (filter), channels_in, H, W)\n",
    "            n_filters, n_channels, _, _ = weights.shape\n",
    "\n",
    "            # Builds a figure\n",
    "            size = (2 * n_channels + 2, 2 * n_filters)\n",
    "            fig, axes = plt.subplots(n_filters, n_channels, \n",
    "                                     figsize=size)\n",
    "            axes = np.atleast_2d(axes)\n",
    "            axes = axes.reshape(n_filters, n_channels)\n",
    "            # For each channel_out (filter)\n",
    "            for i in range(n_filters):    \n",
    "                StepByStep._visualize_tensors(\n",
    "                    axes[i, :],\n",
    "                    weights[i],\n",
    "                    layer_name=f'Filter #{i}', \n",
    "                    title='Channel'\n",
    "                )\n",
    "                    \n",
    "            for ax in axes.flat:\n",
    "                ax.label_outer()\n",
    "\n",
    "            fig.tight_layout()\n",
    "            return fig\n",
    "    except AttributeError:\n",
    "        return\n",
    "    \n",
    "setattr(StepByStep, 'visualize_filters', visualize_filters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = sbs_cnn1.visualize_filters('conv1', cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_model = nn.Linear(1, 1)\n",
    "\n",
    "dummy_list = []\n",
    "\n",
    "def dummy_hook(layer, inputs, outputs):\n",
    "    dummy_list.append((layer, inputs, outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_handle = dummy_model.register_forward_hook(dummy_hook)\n",
    "dummy_handle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_x = torch.tensor([0.3])\n",
    "dummy_model.forward(dummy_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_model(dummy_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_handle.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modules = list(sbs_cnn1.model.named_modules())\n",
    "modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_names = {layer: name for name, layer in modules[1:]}\n",
    "layer_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualization = {}\n",
    "\n",
    "def hook_fn(layer, inputs, outputs):\n",
    "    name = layer_names[layer]\n",
    "    visualization[name] = outputs.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers_to_hook = ['conv1', 'relu1', 'maxp1', 'flatten', 'fc1', 'relu2', 'fc2']\n",
    "\n",
    "handles = {}\n",
    "\n",
    "for name, layer in modules:\n",
    "    if name in layers_to_hook:\n",
    "        handles[name] = layer.register_forward_hook(hook_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_batch, labels_batch = next(iter(val_loader))\n",
    "logits = sbs_cnn1.predict(images_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualization.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for handle in handles.values():\n",
    "    handle.remove()\n",
    "handles = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "setattr(StepByStep, 'visualization', {})\n",
    "setattr(StepByStep, 'handles', {})\n",
    "\n",
    "def attach_hooks(self, layers_to_hook, hook_fn=None):\n",
    "    # Clear any previous values\n",
    "    self.visualization = {}\n",
    "    # Creates the dictionary to map layer objects to their names\n",
    "    modules = list(self.model.named_modules())\n",
    "    layer_names = {layer: name for name, layer in modules[1:]}\n",
    "\n",
    "    if hook_fn is None:\n",
    "        # Hook function to be attached to the forward pass\n",
    "        def hook_fn(layer, inputs, outputs):\n",
    "            # Gets the layer name\n",
    "            name = layer_names[layer]\n",
    "            # Detaches outputs\n",
    "            values = outputs.detach().cpu().numpy()\n",
    "            # Since the hook function may be called multiple times\n",
    "            # for example, if we make predictions for multiple mini-batches\n",
    "            # it concatenates the results\n",
    "            if self.visualization[name] is None:\n",
    "                self.visualization[name] = values\n",
    "            else:\n",
    "                self.visualization[name] = np.concatenate([self.visualization[name], values])\n",
    "\n",
    "    for name, layer in modules:\n",
    "        # If the layer is in our list\n",
    "        if name in layers_to_hook:\n",
    "            # Initializes the corresponding key in the dictionary\n",
    "            self.visualization[name] = None\n",
    "            # Register the forward hook and keep the handle in another dict\n",
    "            self.handles[name] = layer.register_forward_hook(hook_fn)\n",
    "\n",
    "def remove_hooks(self):\n",
    "    # Loops through all hooks and removes them\n",
    "    for handle in self.handles.values():\n",
    "        handle.remove()\n",
    "    # Clear the dict, as all hooks have been removed\n",
    "    self.handles = {}\n",
    "    \n",
    "setattr(StepByStep, 'attach_hooks', attach_hooks)\n",
    "setattr(StepByStep, 'remove_hooks', remove_hooks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbs_cnn1.attach_hooks(layers_to_hook=['conv1', 'relu1', 'maxp1', 'flatten', 'fc1', 'relu2', 'fc2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_batch, labels_batch = next(iter(val_loader))\n",
    "logits = sbs_cnn1.predict(images_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbs_cnn1.remove_hooks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = np.argmax(logits, 1)\n",
    "predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Feature Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_images(images_batch.squeeze(), labels_batch.squeeze(), n_plot=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_outputs(self, layers, n_images=10, y=None, yhat=None):\n",
    "    layers = filter(lambda l: l in self.visualization.keys(), layers)\n",
    "    layers = list(layers)\n",
    "    shapes = [self.visualization[layer].shape for layer in layers]\n",
    "    n_rows = [shape[1] if len(shape) == 4 else 1 \n",
    "              for shape in shapes]\n",
    "    total_rows = np.sum(n_rows)\n",
    "\n",
    "    fig, axes = plt.subplots(total_rows, n_images, \n",
    "                             figsize=(1.5*n_images, 1.5*total_rows))\n",
    "    axes = np.atleast_2d(axes).reshape(total_rows, n_images)\n",
    "    \n",
    "    # Loops through the layers, one layer per row of subplots\n",
    "    row = 0\n",
    "    for i, layer in enumerate(layers):\n",
    "        start_row = row\n",
    "        # Takes the produced feature maps for that layer\n",
    "        output = self.visualization[layer]\n",
    "            \n",
    "        is_vector = len(output.shape) == 2\n",
    "        \n",
    "        for j in range(n_rows[i]):\n",
    "            StepByStep._visualize_tensors(\n",
    "                axes[row, :],\n",
    "                output if is_vector else output[:, j].squeeze(),\n",
    "                y, \n",
    "                yhat, \n",
    "                layer_name=layers[i] \\\n",
    "                           if is_vector \\\n",
    "                           else f'{layers[i]}\\nfil#{row-start_row}',\n",
    "                title='Image' if (row == 0) else None\n",
    "            )\n",
    "            row += 1\n",
    "            \n",
    "    for ax in axes.flat:\n",
    "        ax.label_outer()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "setattr(StepByStep, 'visualize_outputs', visualize_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurizer_layers = ['conv1', 'relu1', 'maxp1', 'flatten']\n",
    "\n",
    "with plt.style.context('seaborn-white'):\n",
    "    fig = sbs_cnn1.visualize_outputs(featurizer_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Classifier Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_layers = ['fc1', 'relu2', 'fc2']\n",
    "\n",
    "with plt.style.context('seaborn-white'):\n",
    "    fig = sbs_cnn1.visualize_outputs(classifier_layers, y=labels_batch, yhat=predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct(self, x, y, threshold=.5):\n",
    "    self.model.eval()\n",
    "    yhat = self.model(x.to(self.device))\n",
    "    y = y.to(self.device)\n",
    "    self.model.train()\n",
    "    \n",
    "    # We get the size of the batch and the number of classes \n",
    "    # (only 1, if it is binary)\n",
    "    n_samples, n_dims = yhat.shape\n",
    "    if n_dims > 1:        \n",
    "        # In a multiclass classification, the biggest logit\n",
    "        # always wins, so we don't bother getting probabilities\n",
    "        \n",
    "        # This is PyTorch's version of argmax, \n",
    "        # but it returns a tuple: (max value, index of max value)\n",
    "        _, predicted = torch.max(yhat, 1)\n",
    "    else:\n",
    "        n_dims += 1\n",
    "        # In binary classification, we NEED to check if the\n",
    "        # last layer is a sigmoid (and then it produces probs)\n",
    "        if isinstance(self.model, nn.Sequential) and \\\n",
    "           isinstance(self.model[-1], nn.Sigmoid):\n",
    "            predicted = (yhat > threshold).long()\n",
    "        # or something else (logits), which we need to convert\n",
    "        # using a sigmoid\n",
    "        else:\n",
    "            predicted = (F.sigmoid(yhat) > threshold).long()\n",
    "    \n",
    "    # How many samples got classified correctly for each class\n",
    "    result = []\n",
    "    for c in range(n_dims):\n",
    "        n_class = (y == c).sum().item()\n",
    "        n_correct = (predicted[y == c] == c).sum().item()\n",
    "        result.append((n_correct, n_class))\n",
    "    return torch.tensor(result)\n",
    "\n",
    "setattr(StepByStep, 'correct', correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbs_cnn1.correct(images_batch, labels_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loader Apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@staticmethod\n",
    "def loader_apply(loader, func, reduce='sum'):\n",
    "    results = [func(x, y) for i, (x, y) in enumerate(loader)]\n",
    "    results = torch.stack(results, axis=0)\n",
    "\n",
    "    if reduce == 'sum':\n",
    "        results = results.sum(axis=0)\n",
    "    elif reduce == 'mean':\n",
    "        results = results.float().mean(axis=0)\n",
    "    \n",
    "    return results\n",
    "\n",
    "setattr(StepByStep, 'loader_apply', loader_apply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "StepByStep.loader_apply(sbs_cnn1.val_loader, sbs_cnn1.correct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting It All Together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Builds tensors from numpy arrays BEFORE split\n",
    "# Modifies the scale of pixel values from [0, 255] to [0, 1]\n",
    "x_tensor = torch.as_tensor(images / 255).float()\n",
    "y_tensor = torch.as_tensor(labels).long()\n",
    "\n",
    "# Uses index_splitter to generate indices for training and\n",
    "# validation sets\n",
    "train_idx, val_idx = index_splitter(len(x_tensor), [80, 20])\n",
    "# Uses indices to perform the split\n",
    "x_train_tensor = x_tensor[train_idx]\n",
    "y_train_tensor = y_tensor[train_idx]\n",
    "x_val_tensor = x_tensor[val_idx]\n",
    "y_val_tensor = y_tensor[val_idx]\n",
    "\n",
    "# We're not doing any data augmentation now\n",
    "train_composer = Compose([Normalize(mean=(.5,), std=(.5,))])\n",
    "val_composer = Compose([Normalize(mean=(.5,), std=(.5,))])\n",
    "\n",
    "# Uses custom dataset to apply composed transforms to each set\n",
    "train_dataset = TransformedTensorDataset(x_train_tensor, y_train_tensor, transform=train_composer)\n",
    "val_dataset = TransformedTensorDataset(x_val_tensor, y_val_tensor, transform=val_composer)\n",
    "\n",
    "# Builds a weighted random sampler to handle imbalanced classes\n",
    "sampler = make_balanced_sampler(y_train_tensor)\n",
    "\n",
    "# Uses sampler in the training set to get a balanced data loader\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=16, sampler=sampler)\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(13)\n",
    "model_cnn1 = nn.Sequential()\n",
    "\n",
    "# Featurizer\n",
    "# Block 1: 1@10x10 -> n_channels@8x8 -> n_channels@4x4\n",
    "n_channels = 1\n",
    "model_cnn1.add_module('conv1', nn.Conv2d(in_channels=1, out_channels=n_channels, kernel_size=3))\n",
    "model_cnn1.add_module('relu1', nn.ReLU())\n",
    "model_cnn1.add_module('maxp1', nn.MaxPool2d(kernel_size=2))\n",
    "# Flattening: n_channels * 4 * 4\n",
    "model_cnn1.add_module('flatten', nn.Flatten())\n",
    "\n",
    "# Classification\n",
    "# Hidden Layer\n",
    "model_cnn1.add_module('fc1', nn.Linear(in_features=n_channels*4*4, out_features=10))\n",
    "model_cnn1.add_module('relu2', nn.ReLU())\n",
    "# Output Layer\n",
    "model_cnn1.add_module('fc2', nn.Linear(in_features=10, out_features=3))\n",
    "\n",
    "lr = 0.1\n",
    "multi_loss_fn = nn.CrossEntropyLoss(reduction='mean')\n",
    "optimizer_cnn1 = optim.SGD(model_cnn1.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbs_cnn1 = StepByStep(model_cnn1, multi_loss_fn, optimizer_cnn1)\n",
    "sbs_cnn1.set_loaders(train_loader, val_loader)\n",
    "sbs_cnn1.train(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_filters = sbs_cnn1.visualize_filters('conv1', cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Capturing Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurizer_layers = ['conv1', 'relu1', 'maxp1', 'flatten']\n",
    "classifier_layers = ['fc1', 'relu2', 'fc2']\n",
    "\n",
    "sbs_cnn1.attach_hooks(layers_to_hook=featurizer_layers + classifier_layers)\n",
    "\n",
    "images_batch, labels_batch = next(iter(val_loader))\n",
    "logits = sbs_cnn1.predict(images_batch)\n",
    "predicted = np.argmax(logits, 1)\n",
    "\n",
    "sbs_cnn1.remove_hooks()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Feature Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with plt.style.context('seaborn-white'):\n",
    "    fig_maps1 = sbs_cnn1.visualize_outputs(featurizer_layers)\n",
    "    fig_maps2 = sbs_cnn1.visualize_outputs(classifier_layers, y=labels_batch, yhat=predicted)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "StepByStep.loader_apply(sbs_cnn1.val_loader, sbs_cnn1.correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
